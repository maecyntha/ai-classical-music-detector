# ai-classical-music-detector

This repository contains code and notebooks to detect whether a piece of classical music was composed by a human or generated by AI. The detection is performed using deep learning models, including LSTM and CNN architectures, trained on symbolic music representations (MIDI files).

### Data
In this repository, raw datasets are located in the `data/raw` directory. These include:
- Hugging Face dataset - https://huggingface.co/datasets/drengskapur/midi-classical-music
- JS Fake Chorales dataset - https://github.com/omarperacha/js-fakes

In addition to the above, you may also include your own dataset in a similar format.

Supported formats for custom datasets:
- Zipped MIDI files
- Linked JSONL files containing symbolic music data


### Notebooks
There are three main Jupyter notebooks in the `notebooks/` directory that make up the AI classical music detector pipeline:
- Feature Extraction - `notebooks/01_feature_extraction.ipynb` <br />
    his notebook handles the extraction of statistical and expressive features from symbolic music files. These features are used as input for the machine learning models.

- LSTM-based Model - `notebooks/02_lstm_ai_music_detector.ipynb` <br />
    An LSTM-based classifier that learns temporal patterns in the music segments and distinguishes AI-generated pieces from human compositions.

- CNN-based Model - `notebooks/03_cnn_ai_music_detector.ipynb` <br />
    A lightweight convolutional model that captures local patterns in musical features, providing an alternative to LSTM for faster training and inference.

### Usage

1. **Prepare the Dataset** <br />
    To make a dataset from the data you have collected, use `01_feature_extraction` notebook to process your files and combine them into a dataset.

    - Currently, the format of data that can be proceed is limited to zipped MIDI and linked JSONL.
    - Define file paths or URLs in the last cell.
    - Set `max_segment` parameter to determine the number of segments per sample for consistent input dimensions. This might be helpful when you tried to make an auxiliary dataset.

2. **Train the Model** <br />
    After the dataset has been prepared,
    - Use `02_cnn_ai_music_detector` to train an LSTM-based model.
    - Use `03_cnn_ai_music_detector.ipynb` to train a CNN-based model.

3. **Saving the Model**
    Each training notebook provides functionality to save the best-performing model during training. You can later load this model for evaluation or inference.

###  Notes
Feature extraction includes statistical measures like pitch, velocity, and duration mean/median/std per segment, as well as global features like tempo, articulation, and rhythmic density.

Segmentation and padding strategies ensure uniform input size to allow batch training across diverse pieces of music.
